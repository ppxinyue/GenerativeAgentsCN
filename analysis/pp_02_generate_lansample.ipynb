{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3323def5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AA...\n",
      "Processing BB...\n",
      "Processing CC...\n",
      "Processing DD...\n",
      "Processing EE...\n",
      "Processing FF...\n",
      "Processing GG...\n",
      "Processing HH...\n",
      "Processing II...\n",
      "Processing JJ...\n",
      "Processing KK...\n",
      "Processing LL...\n",
      "Processing MM...\n",
      "Processing NN...\n",
      "Processing OO...\n",
      "Processing PP...\n",
      "Processing QQ...\n",
      "Processing MissT...\n",
      "Output saved to gpt-4o-mini_sentences.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "\n",
    "# Prompt user to input API key and base URL\n",
    "api_key = \"sk-3ijUbpq1p9DtmydgTk0Rr1cWRZVbRtT8QlZ2nv2wXZgZ6P9x\"\n",
    "base_url = \"https://api.openai-proxy.org/v1\"\n",
    "\n",
    "# Initialize OpenAI client with provided API key and base URL\n",
    "client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "\n",
    "# List of personas\n",
    "personas = [\"AA\", \"BB\", \"CC\", \"DD\", \"EE\", \"FF\", \"GG\", \"HH\", \n",
    "            \"II\", \"JJ\", \"KK\", \"LL\", \"MM\", \"NN\", \"OO\", \"PP\", \n",
    "            \"QQ\", \"MissT\"]\n",
    "\n",
    "# Base directory for agent JSON files\n",
    "BASE_DIR = r\"D:\\ppXinyue\\2022_inclusion\\ppCNtown\\GenerativeAgentsCN\\generative_agents\\frontend\\static\\assets\\village\\agents\"\n",
    "\n",
    "# Function to read scratch from JSON file\n",
    "def read_scratch(persona):\n",
    "    file_path = Path(BASE_DIR) / f\"{persona}\" / \"agent.json\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            return data.get(\"scratch\", \"\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to generate 10 sentences using OpenAI API\n",
    "def generate_sentences(scratch, persona):\n",
    "    prompt = f\"Based on the following character description:\\n{scratch}\\nGenerate 10 sentences that this character ({persona}) might say, reflecting their personality and context.\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a creative assistant generating dialogue for a character based on their description.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        # Split response into sentences and ensure exactly 10\n",
    "        sentences = response.choices[0].message.content.strip().split('\\n')\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        return sentences[:10] if len(sentences) >= 10 else sentences + [\"\"] * (10 - len(sentences))\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating sentences for {persona}: {e}\")\n",
    "        return [\"\"] * 10\n",
    "\n",
    "# Prepare CSV file\n",
    "output_file = \"gpt-4o-mini_sentences.csv\"\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write header\n",
    "    writer.writerow([\"Persona\", \"Sentence\"])\n",
    "    \n",
    "    # Process each persona\n",
    "    for persona in personas:\n",
    "        print(f\"Processing {persona}...\")\n",
    "        scratch = read_scratch(persona)\n",
    "        if scratch:\n",
    "            sentences = generate_sentences(scratch, persona)\n",
    "            for sentence in sentences:\n",
    "                writer.writerow([persona, sentence])\n",
    "        else:\n",
    "            for _ in range(10):\n",
    "                writer.writerow([persona, \"\"])\n",
    "\n",
    "print(f\"Output saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5afa174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AA...\n",
      "Processing BB...\n",
      "Processing CC...\n",
      "Processing DD...\n",
      "Processing EE...\n",
      "Processing FF...\n",
      "Processing GG...\n",
      "Processing HH...\n",
      "Processing II...\n",
      "Processing JJ...\n",
      "Processing KK...\n",
      "Processing LL...\n",
      "Processing MM...\n",
      "Processing NN...\n",
      "Processing OO...\n",
      "Processing PP...\n",
      "Processing QQ...\n",
      "Processing MissT...\n",
      "Output saved to glm4_sentences.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "# Prompt user to input API key\n",
    "api_key = \"501e715297d14d5ab6e1626469f6c031.ZtiLDCRLSSyijhEC\"  # Replace with your ZhipuAI API key\n",
    "base_url = \"https://open.bigmodel.cn/api/paas/v4\"  # ZhipuAI base URL\n",
    "# ZhipuAI does not require a base_url as it's handled by the client\n",
    "\n",
    "# Initialize ZhipuAI client with provided API key\n",
    "client = ZhipuAI(api_key=api_key, base_url=base_url)\n",
    "\n",
    "# List of personas\n",
    "personas = [\"AA\", \"BB\", \"CC\", \"DD\", \"EE\", \"FF\", \"GG\", \"HH\", \n",
    "            \"II\", \"JJ\", \"KK\", \"LL\", \"MM\", \"NN\", \"OO\", \"PP\", \n",
    "            \"QQ\", \"MissT\"]\n",
    "\n",
    "# Base directory for agent JSON files\n",
    "BASE_DIR = r\"D:\\ppXinyue\\2022_inclusion\\ppCNtown\\GenerativeAgentsCN\\generative_agents\\frontend\\static\\assets\\village\\agents\"\n",
    "\n",
    "# Function to read scratch from JSON file\n",
    "def read_scratch(persona):\n",
    "    file_path = Path(BASE_DIR) / f\"{persona}\" / \"agent.json\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            return data.get(\"scratch\", \"\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to generate 10 sentences using ZhipuAI API\n",
    "def generate_sentences(scratch, persona):\n",
    "    prompt = f\"Based on the following character description:\\n{scratch}\\nGenerate 10 sentences that this character ({persona}) might say, reflecting their personality and context.\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"glm-4\",  # Use ZhipuAI's glm-4 model (or another available model)\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a creative assistant generating dialogue for a character based on their description.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        # Split response into sentences and ensure exactly 10\n",
    "        sentences = response.choices[0].message.content.strip().split('\\n')\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        return sentences[:10] if len(sentences) >= 10 else sentences + [\"\"] * (10 - len(sentences))\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating sentences for {persona}: {e}\")\n",
    "        return [\"\"] * 10\n",
    "\n",
    "# Prepare CSV file\n",
    "output_file = \"glm4_sentences.csv\"\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write header\n",
    "    writer.writerow([\"Persona\", \"Sentence\"])\n",
    "    \n",
    "    # Process each persona\n",
    "    for persona in personas:\n",
    "        print(f\"Processing {persona}...\")\n",
    "        scratch = read_scratch(persona)\n",
    "        if scratch:\n",
    "            sentences = generate_sentences(scratch, persona)\n",
    "            for sentence in sentences:\n",
    "                writer.writerow([persona, sentence])\n",
    "        else:\n",
    "            for _ in range(10):\n",
    "                writer.writerow([persona, \"\"])\n",
    "\n",
    "print(f\"Output saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d8b52234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_36228\\3191739493.py:49: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(x='Model', y='Authenticity', data=data, order=model_order, palette=['lightblue'], width=0.85)\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_36228\\3191739493.py:49: UserWarning: \n",
      "The palette list has fewer values (1) than needed (6) and will cycle, which may produce an uninterpretable plot.\n",
      "  sns.boxplot(x='Model', y='Authenticity', data=data, order=model_order, palette=['lightblue'], width=0.85)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Loading and cleaning data\n",
    "def load_and_clean_data():\n",
    "    data = pd.read_csv('model_rating.csv')\n",
    "    # data = data[data['rater'] == 'sx']\n",
    "    data = data.dropna()\n",
    "    data['Authenticity'] = pd.to_numeric(data['Authenticity'], errors='coerce')\n",
    "    data['AI'] = pd.to_numeric(data['AI'], errors='coerce')\n",
    "    data['AUTISM'] = pd.to_numeric(data['AUTISM'], errors='coerce')\n",
    "    data['answer_AI'] = pd.to_numeric(data['answer_AI'], errors='coerce')\n",
    "    data['answer_AUTISM'] = pd.to_numeric(data['answer_AUTISM'], errors='coerce')\n",
    "    return data\n",
    "\n",
    "# Calculating accuracy for AUTISM judgments and proportions for AI identification\n",
    "def calculate_accuracies_and_proportions(data):\n",
    "    # Calculate AUTISM accuracy\n",
    "    data['AUTISM_Accuracy'] = (data['AUTISM'] == data['answer_AUTISM']).astype(int)\n",
    "    return data\n",
    "\n",
    "# Preparing data for stacked bar charts\n",
    "def prepare_stacked_bar_data(data):\n",
    "    # Calculate proportions for AI identification (AI == 1 vs AI == 0)\n",
    "    ai_proportions = data.groupby('Model')['AI'].value_counts(normalize=True).unstack().fillna(0)\n",
    "    ai_proportions.columns = ['Identified_Children', 'Identified_AI']\n",
    "    # Sort by Identified_AI in descending order\n",
    "    ai_proportions = ai_proportions.sort_values('Identified_Children', ascending=False)\n",
    "    \n",
    "    # Calculate proportions for AUTISM_Accuracy\n",
    "    autism_accuracy_counts = data.groupby('Model')['AUTISM_Accuracy'].value_counts(normalize=True).unstack().fillna(0)\n",
    "    autism_accuracy_counts.columns = ['AUTISM_Incorrect', 'AUTISM_Correct']\n",
    "    # Sort by AUTISM_Correct in descending order\n",
    "    autism_accuracy_counts = autism_accuracy_counts.sort_values('AUTISM_Correct', ascending=False)\n",
    "    \n",
    "    return ai_proportions, autism_accuracy_counts\n",
    "\n",
    "# Plotting box plot for Authenticity and stacked bar charts for AI and AUTISM\n",
    "def plot_charts(data, ai_proportions, autism_accuracy_counts):\n",
    "    sns.set(style=\"white\")\n",
    "    \n",
    "    # Calculate mean Authenticity scores per model for sorting\n",
    "    model_order = data.groupby('Model')['Authenticity'].mean().sort_values(ascending=False).index\n",
    "    \n",
    "    # Plot 1: Authenticity scores by model\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.boxplot(x='Model', y='Authenticity', data=data, order=model_order, palette=['lightblue'], width=0.85)\n",
    "    sns.stripplot(x='Model', y='Authenticity', data=data, order=model_order, color='lightblue', size=10, marker='o', \n",
    "                  jitter=True, alpha=0.9, edgecolor='black', linewidth=1)\n",
    "    plt.title('Authenticity Scores by Model', fontsize=26, pad=10)\n",
    "    plt.ylabel('Authenticity Score', fontsize=26)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=26)\n",
    "    plt.yticks(fontsize=26)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('authenticity_boxplot.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    # Plot 2: Stacked bar chart for AI identification proportion\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    ai_proportions[['Identified_Children', 'Identified_AI']].plot(kind='bar', stacked=True, color=['#00C9BB', '#EF0C99'], width=0.9, alpha=0.6)\n",
    "    plt.title('Proportion of AI Identification by Model', fontsize=16)\n",
    "    plt.xlabel('Model', fontsize=16)\n",
    "    plt.ylabel('Proportion', fontsize=16)\n",
    "    plt.legend(['Identified_Children', 'Identified_AI'], title='AI Identification', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=16)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ai_identification_stacked_bar.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 3: Stacked bar chart for AUTISM judgment accuracy (unchanged)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    autism_accuracy_counts[['AUTISM_Correct', 'AUTISM_Incorrect']].plot(kind='bar', stacked=True, color=['#00C9BB', '#EF0C99'], width=0.9, alpha=0.6)\n",
    "    plt.title('AUTISM Judgment Accuracy by Model', fontsize=16)\n",
    "    plt.xlabel('Model', fontsize=16)\n",
    "    plt.ylabel('Proportion', fontsize=16)\n",
    "    plt.legend(['Correct', 'Incorrect'], title='AUTISM Judgment', fontsize=16)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('autism_accuracy_stacked_bar.png')\n",
    "    plt.close()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and clean the data\n",
    "    data = load_and_clean_data()\n",
    "    \n",
    "    # Calculate accuracies and proportions\n",
    "    data = calculate_accuracies_and_proportions(data)\n",
    "    \n",
    "    # Prepare data for stacked bar charts\n",
    "    ai_proportions, autism_accuracy_counts = prepare_stacked_bar_data(data)\n",
    "    \n",
    "    # Generate plots\n",
    "    plot_charts(data, ai_proportions, autism_accuracy_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7de3f592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?asdsada上帝\n",
      "Hello\n",
      "世界\n",
      "{'greeting': None, 'language': None}\n",
      "Hello\n",
      "世界\n",
      "[None, None]\n"
     ]
    }
   ],
   "source": [
    "def unicode_escape(msg):\n",
    "    if isinstance(msg, str):\n",
    "        print(msg.encode('gbk', errors='replace').decode('gbk'))\n",
    "    elif isinstance(msg, dict):\n",
    "        print({k: unicode_escape(v) for k, v in msg.items()})\n",
    "    elif isinstance(msg, list):\n",
    "        print([unicode_escape(v) for v in msg])\n",
    "    else:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    example_string = \"⚽asdsada上帝\"\n",
    "    example_dict = {\"greeting\": \"Hello\", \"language\": \"世界\"}\n",
    "    example_list = [\"Hello\", \"世界\"]\n",
    "    \n",
    "    unicode_escape(example_string)\n",
    "    unicode_escape(example_dict)\n",
    "    unicode_escape(example_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inclusive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
